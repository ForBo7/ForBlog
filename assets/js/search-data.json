{
  
    
        "post0": {
            "title": "📌 Detecting Floods for Disaster Relief",
            "content": "You can find this notebook on Kaggle here. . . The model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan. . Such models can prove useful in flood relief, helping to detect which areas need immediate focus. . The dataset used to train this model is Louisiana flood 2016, uploaded by Kaggle user Rahul T P, which you can view here. . The fastai library, a high level PyTorch library, has been used. . One of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is not a tutorial or guide. . from fastai.vision.all import * . Sort data. . The data in the dataset needs to be organized into train and valid folders. Each will contain the same subfolders, 0 and 1, which will be used to label the data. A label of 0 indicates the area shown in the image is not flooded, while a label of 1 indicates the area shown in the image is flooded. . The images in the dataset itself has been organized as follows: . &nbsp;&nbsp;&nbsp;&nbsp;If no underscore is in the file name, the image shows the area before or after the flood. . &nbsp;&nbsp;&nbsp;&nbsp;If an underscore is in the file name, the image shows the area during the flood: . If a zero follows the underscore, the area was not flooded. | If a one follows the underscore, the area was flooded. | . Creating the necessary paths. . working_path = Path.cwd(); print(working_path) folders = (&#39;train&#39;, &#39;valid&#39;) labels = (&#39;0&#39;, &#39;1&#39;) . /kaggle/working . input_path = Path(&#39;/kaggle/input&#39;) train_image_paths = sorted(input_path.rglob(&#39;train/*.png&#39;)) valid_image_paths = sorted(input_path.rglob(&#39;test/*.png&#39;)) len(train_image_paths), len(valid_image_paths) . (270, 52) . Creating the necessary directories. . for folder in folders: if not (working_path/folder).exists(): (working_path/folder).mkdir() for label in labels: if not (working_path/folder/label).exists(): (working_path/folder/label).mkdir() . Move images to new directories. . try: for image_path in train_image_paths: if &#39;_1&#39; in image_path.stem: with (working_path/&#39;train&#39;/&#39;1&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) else: with (working_path/&#39;train&#39;/&#39;0&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) except FileExistsError: print(&quot;Training images have already been moved.&quot;) else: print(&quot;Training images moved.&quot;) . Training images moved. . try: for image_path in valid_image_paths: if &#39;_1&#39; in image_path.stem: with (working_path/&#39;valid&#39;/&#39;1&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) else: with (working_path/&#39;valid&#39;/&#39;0&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) except FileExistsError: print(&quot;Testing images have already been moved.&quot;) else: print(&quot;Testing images moved.&quot;) . Testing images moved. . Check that images have been moved. . training_images = get_image_files(working_path/&#39;train&#39;); print(len(training_images)) . 270 . Image.open(training_images[0]) . validation_images = get_image_files(working_path/&#39;valid&#39;); print(len(validation_images)) . 52 . Image.open(validation_images[-1]) . Load data . Create the training and validation dataloaders through fastai&#39;s quick and easy DataBlock class. . dataloaders = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = GrandparentSplitter(), get_y = parent_label, item_tfms = [Resize(192, method=&#39;squish&#39;)] ).dataloaders(working_path, bs=32) . Check that data has been loaded correctly. . dataloaders.show_batch(max_n=8) . Instantiate and Train Model . learner = vision_learner(dataloaders, resnet18, metrics=error_rate) learner.fine_tune(9) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 0.919323 | 1.118264 | 0.365385 | 00:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.490039 | 0.628054 | 0.250000 | 00:02 | . 1 | 0.367996 | 0.411558 | 0.192308 | 00:02 | . 2 | 0.266664 | 0.472146 | 0.192308 | 00:02 | . 3 | 0.203069 | 0.256436 | 0.115385 | 00:03 | . 4 | 0.158453 | 0.127106 | 0.076923 | 00:03 | . 5 | 0.124499 | 0.095927 | 0.038462 | 00:02 | . 6 | 0.098409 | 0.089279 | 0.038462 | 00:03 | . 7 | 0.079600 | 0.093277 | 0.038462 | 00:02 | . 8 | 0.064886 | 0.090372 | 0.038462 | 00:02 | . Nice! A relatively low error rate for no tweaking. . Visualizing Mistakes . We have to see how the model is getting confuzzled. . interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix() . Only a couple of mistakes. Let&#39;s see what they are. . interp.plot_top_losses(5, nrows=1) . Nothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes. . Model Inference . Let&#39;s test the model on some images of the recent flooding in Pakistan. . def infer_image(image_path): display(Image.open(image_path)) label, _, probabilities = learner.predict(PILImage(PILImage.create(image_path))) if label == &#39;0&#39;: print(f&quot;The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.&quot;) elif label == &#39;1&#39;: print(f&quot;The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.&quot;) else: print(&quot;Unknown label assigned to image.&quot;) . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;1.jpeg&#39;) . The area shown in the image is not flooded with probability 65.65%. . Not bad! . Let&#39;s try it on another image. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;2.jpg&#39;) . The area shown in the image is flooded with probability 99.90%. . The label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;3.jpg&#39;) . The area shown in the image is flooded with probability 99.99%. . The model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;) . The area shown in the image is not flooded with probability 64.56%. . Over here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down. . Let&#39;s do two more images. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;5.jpg&#39;) . The area shown in the image is flooded with probability 99.94%. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;6.jpg&#39;) . The area shown in the image is flooded with probability 100.00%. . The model is working well with images of different sizes too, and has given this image a very high, correct confidence. . Improving the model. . Let&#39;s see if we can get the model&#39;s performance to improve on the image the following image through augmenting the training set. . Image.open(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;) . augmented_dataloaders = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = GrandparentSplitter(), get_y = parent_label, item_tfms = RandomResizedCrop(192, min_scale=0.5), batch_tfms=aug_transforms() ).dataloaders(working_path, bs=32) . augmented_dataloaders.show_batch(max_n=8) . augmented_learner = vision_learner(augmented_dataloaders, resnet18, metrics=error_rate) augmented_learner.fine_tune(9) . epoch train_loss valid_loss error_rate time . 0 | 1.161182 | 0.835870 | 0.365385 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.442552 | 0.686252 | 0.288462 | 00:03 | . 1 | 0.417739 | 0.411907 | 0.153846 | 00:02 | . 2 | 0.346400 | 0.316388 | 0.057692 | 00:03 | . 3 | 0.306782 | 0.213407 | 0.076923 | 00:02 | . 4 | 0.251947 | 0.199586 | 0.076923 | 00:02 | . 5 | 0.209951 | 0.141818 | 0.057692 | 00:02 | . 6 | 0.188433 | 0.116713 | 0.057692 | 00:03 | . 7 | 0.169689 | 0.125078 | 0.057692 | 00:02 | . 8 | 0.151843 | 0.131188 | 0.057692 | 00:02 | . Let&#39;s try the new model out. . display(Image.open(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;)) label, _, probabilities = augmented_learner.predict(PILImage(PILImage.create(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;))) if label == &#39;0&#39;: print(f&quot;The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.&quot;) elif label == &#39;1&#39;: print(f&quot;The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.&quot;) else: print(&quot;Unknown label assigned to image.&quot;) . The area shown in the image is flooded with probability 99.91%. . Dang, impressive! The correct label and with excellent confidence! . Before we get too excited though, we should check the performance on the model with the previous images. . test_dataloader = learner.dls.test_dl([image_path for image_path in sorted((input_path/&#39;floodclassifiertestset&#39;).rglob(&#39;*.*&#39;))]) . probabilities, _, labels = augmented_learner.get_preds(dl=test_dataloader, with_decoded=True) . print(&quot;Images are numbered horizontally.&quot;) test_dataloader.show_batch() for probability, label, image_number in zip(probabilities, labels, range(1, 7)): if label == 1: print(f&quot;Image {image_number} is flooded with a probability of {probability[1]*100:.2f}%.&quot;) elif label == 0: print(f&quot;Image {image_number} is not flooded with a probability of {probability[0]*100:.2f}%.&quot;) else: print(f&quot;Image {image_number} has been assigned an unknown label.&quot;) . Images are numbered horizontally. Image 1 is flooded with a probability of 95.94%. Image 2 is flooded with a probability of 99.92%. Image 3 is flooded with a probability of 91.34%. Image 4 is flooded with a probability of 99.71%. Image 5 is flooded with a probability of 100.00%. Image 6 is flooded with a probability of 100.00%. . Drastically improved probabilities! A little augmentation can go a long way. . Takeaways . This model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half. . This is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions. . However, there could be some improvements and additions made to the model: . Include a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images. | Train the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features. | . If you have any comments, suggestions, feedback, criticism, or improvements, please do post them down in the comment section below!. .",
            "url": "https://forbo7.github.io/ForBlog/fastai/image%20classification/2022/09/12/Detecting-Floods-for-Disaster-Relief.html",
            "relUrl": "/fastai/image%20classification/2022/09/12/Detecting-Floods-for-Disaster-Relief.html",
            "date": " • Sep 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Katakana",
            "content": ". You can check out my Hiragana post here. . Katakana is the Japanese writing system that is mainly used for loan words and foriegn names. Katakana for the most part works exactly like Hiragana. . Katakana Syllables . Every Hiragana syllable has a Katakana counterpart. There are 46 main Katakana syllables, though more exist which I will go over in a later section. .   | a | i | u | e | o | .   | ア | イ | ウ | エ | オ | . k | カ | キ | ク | ケ | コ | . s | サ | シ (shi) | ス | セ | ソ | . t | タ | チ (chi) | ツ (tsu) | テ | ト | . n | ナ | ニ | ヌ | ネ | ノ | . h | ハ | ヒ | フ (fu) | ヘ | ホ | . m | マ | ミ | ム | メ | モ | . y | ヤ |   | ユ |   | ヨ | . r | ラ | リ | ル | レ | ロ | . w | ワ |   |   |   | ヲ (o/wo) | . n | ン | . シ, チ, ツ, and フ sound different and is shown in brackets. However, the romanization is only an approximation. . シ and チ can be described to be shallower versions of sh and ch. . ツ can be described to have an initial light t sound. . フ can be described to be a heavy hu sound that is a blend between hu and fu. . Observations . These characters are a lot more “sharp” and straight. They also feel a lot more like Chinese characters. . シ and ツ look really similar too. How I like remembering them is that シ is more shallow whereas ツ is more deep. . Similarly, ソ is more deep whereas ン is more shallow. . Katakana with Diacritical Marks . Just like Hiragana, a further 25 syllables can be derived with the diacritic marks ゛and ゜. However, there are a few extra variations which I will get to in a later section. .   | a | i | u | e | o | . k | カ | キ | ク | ケ | コ | . g | ガ | ギ | グ | ゲ | ゴ | . s | サ | シ (shi) | ス | セ | ソ | . z | ザ | ジ (ji) | ズ | ゼ | ゾ | . t | タ | チ (chi) | ツ (tsu) | テ | ト | . d | ダ | ヂ (ji) | ヅ (tzu) | デ | ド | . h | ハ | ヒ | フ (fu) | ヘ | ホ | . b | バ | ビ | ブ | ベ | ボ | .   | a | e | u | e | o | . h | ハ | ヒ | フ (fu) | ヘ | ホ | . p | パ | ピ | プ | ペ | ポ | . Transcribing Contracted Sounds . . Again, just like Hiragana, a further 21 syllables can be derived using small ヤ, ユ, and ヨ. However, like before, there are further small vowels that are used which I will discuss in a section below. .   | ya | yu | yo | . k | キャ | キュ | キョ | . s | シャ (sha) | シュ (shu) | ショ (sho) | . t | チャ (cha) | チュ (chu) | チョ (cho) | . n | ニャ | ニュ | ニョ | . h | ヒャ | ヒュ | ヒョ | . m | ミャ | ミュ | ミョ | . r | リャ | リュ | リオ | .   | ya | yu | yo | . g | ギャ | ギュ | ギョ | . z | ジャ (ja) | ジュ (ju) | ジョ (jo) | . b | ビャ | ビュ | ビョ | . p | ピャ | ピュ | ピョ | . The イ vowel syllables are used for contraction. . Double Consonants and Long Vowels . The same rules from Hiragana apply: small ツ is used to transcribe double consonants, except when transcribing double consonant n sounds. Instead, ン is used along with a Katakana that has an initial n sound. . Unlike Hiragana though, when transcribing long vowel sounds, extra vowel syllables are not added. Instead, ー is used. When writing vertically, ー becomes ｜. . Hiragana . おばあさん — obaasan えいが — eega . Katakana . カー — kaa (car) ボール — booru (ball) . Transcribing Foreign Sounds . . Katakana is the writing system used for loan words. As such, not all sounds can be represented in Japanese (such as l). However, extra Katakana characters have been made to accomodate for extra sounds that do not exist through the use of small vowels (ァ、ィ、ゥ、ェ、ォ) and diacritics. .   | a | i | u | e | o | . s |   |   |   | シェ (she) |   | . z |   |   |   | ジェ (je) |   | . t |   | ティ | トゥ |   |   | . d |   | ヂィ | ドゥ |   |   | . h | ファ (fa) | フィ (fi) |   | フェ (fu) | フォ (fo) | . w |   | ウィ | ウ | ウェ | ウォ | . v | ヴァ | ヴィ | ヴ | ヴェ | ヴォ | . Conclusion . After learning both the Hiragana and Katakana, Japanese has come to be a very interesting language due to how words and syllables are structured and by the lack of certain sounds. Japanese feels “basic” (not in a bad way) in the sense that there are much fewer sounds which are also preset. For example, in English, let’s say we have the letter k. You can add any letter you want afterwards. However, in Japanese, you have k that is combined with a vowel; that is, you don’t have k by itself. . Reading English words that have been brought in through Katakana is also quite interesting and amusing at times. .",
            "url": "https://forbo7.github.io/ForBlog/japanese/katakana/2022/08/25/Katakana.html",
            "relUrl": "/japanese/katakana/2022/08/25/Katakana.html",
            "date": " • Aug 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Greetings and Numbers",
            "content": "Greetings / あいさつ . Japanese has various exchanges for various different situations. They are typically accompanied by bowing, which can express gratitude, respect, or apologies. It can range to a slight nod of the head to a 45 degree bow from the waist. The longer and deeper the bow, the more formal and respectful it is. . When meeting in business situations, it is also customary to exchange business cards (めいし). . Various greetings . おはよう is used typically before noon, though it is also used later in the day in casual settings or when classmates or coworkers are met for the first time in the day. . こんにちは is used in the afternoon. . こんばんは is used in the evening or during the night. . おやすみなさい is used as “Goodnight”. . さようなら is a very formal, deepfelt goodbye; akin to how “Farewell” is supposed to be used in English. However, it’s daily use is mainly kept to school children taking leave of thier teachers. . じゃあ、また is a much more casual goodbye statement. It is akin to “see you”. . しつれいします is less formal than さようなら but more formal than じゃあ、また. For example, it can be used when taking leave of a professor’s office. . Exchanges . すみません is used in the following situations: . To gain the attention of someone (akin to “excuse me”) | To apologize | To show appreciation (akin to “thank you”) | . いってきます is spoken when a family member leaves the house. いってらっしゃい is spoken in reponse. . ただいま is spoken when a family member returns home. おかえりなさい is spoken in response. . いってだきます is spoken when you receive a meal. ごちそうさまでして is spoken after you finish a meal. . ございます . ございます is added to certain phrases to make them more polite. For example, おはようございます and ありがとうございます. . Numbers / すうじ . Numbers are easy. Let’s see how. . 0 - 10 . 0 | ゼロ / れい | . 1 | いち | . 2 | に | . 3 | さん | . 4 | よん / し / よ | . 5 | ご | . 6 | ろく | . 7 | なな / しち | . 8 | はち | . 9 | きゅう / く | . 10 | じゅう | . 10 - 19 . Now comes the easy part. To say the teens, you say 10 then the desired number. . 10 | じゅう | . 11 | じゅういち | . 12 | じゅうに | . 13 | じゅうさん | . 14 | じゅうよん　/ じゅうし | . 15 | じゅうご | . 16 | じゅうろく | . 17 | じゅうなな / じゅうしち | . 18 | じゅうはち | . 19 | じゅうきゅう / じゅうく | . 10 - 90 . Then tens are similarly, intuitively structured: you say the desired number then 10. . 10 | じゅう | . 20 | にじゅう | . 30 | さんじゅう | . 40 | よんじゅう | . 50 | ごじゅう | . 60 | ろくじゅう | . 70 | ななじゅう | . 80 | はちじゅう | . 90 | きゅうじゅう | . Conclusion . The Japanese have specific etiquettes for a variety of different situations, each with varying levels of respect and gratitude. However, respect and gratitude is always shown in those exchanges, becoming a part of the mindset and culture. . Numbers are structured like Mandarin and are intuitive too: instead of saying, say, fifteeen, you say ten five. Likewise, instead of saying, say, fourty, you say four ten. .",
            "url": "https://forbo7.github.io/ForBlog/japanese/greetings/numbers/2022/08/18/Greetings-and-Numbers.html",
            "relUrl": "/japanese/greetings/numbers/2022/08/18/Greetings-and-Numbers.html",
            "date": " • Aug 18, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Hiragana",
            "content": ". So begins my journey into Japanese! To start of with, I will be learning the Hiragana alphabet. . Japanese Writing System . There are three writing systems: . Hiragana | Katakana | Kanji | . Hiragana and Katakana are collectively known as Kana. Kana represent phonetic sounds whereas Kanji are Chinese characters. . Hiragana can be identified from its more curvy shapes. It is used for: . conjugation endings | function words | native Japanese words that do not have a Kanji represnetation | . Katakana can be identified from its more straight, sharp shapes. It is used for: . loan words | foreign names | . Kanji is mainly used for: . nouns | stems of verbs | stems of adjectives | . Hiragana Syllables . There are 46 main Hiragana syllables. Some written Hiragana syllables differ from print Hiragana. . . 　 | a | i | u | e | o | .   | あ | い | う | え | お | . k | か | き | く | け | こ | . s | さ | し (shi) | す | せ | そ | . t | た | ち (chi) | つ (tsu) | て | と | . n | な | に | ぬ | ね | の | . h | は | ひ | ふ (fu) | へ | ほ | . m | ま | め | む | め | も | . y | や |   | ゆ |   | よ | . r | ら | り | る | れ | ろ | . w | わ |   |   |   | を (o/wo) | . n | ん | . し, ち, つ, and ふ sound different and is shown in brackets. The romanization in the brackets are only an approximation. . し and ち can be described to be shallower versions of the “sh” and “ch” sounds respectively. . つ can be described to have a light initial “t” sound. . ふ can be described to be a heavy “hu” sound that is a blend between “hu” and “fu”. . Observations . Japanese characters looking different handwritten and typed/print is similar to English. For example, “a” written in English does not have the type while print English does. . Hiragana with Diacritical Marks . . There are 25 further syllables that can be derived with diacrtic marks. . Dakuten (゛) turns the consonants k, s, t, and h into g, z, d, and b respectively. . Handakuten (゜) turns the consonant h into p. . 　 | a | i | u | e | o | .   | あ | い | う | え | お | . k | か | き | く | け | こ | . g | が | ぎ | ぐ | げ | ご | . s | さ | し (shi) | す | せ | そ | . z | ざ | じ (ji) | ず | ぜ | ぞ | . t | た | ち (chi) | つ (tsu) | て | と | . d | だ | ぢ　(ji) | づ (tzu) | で | ど | . h | は | ひ | ふ (fu) | へ | ほ | . b | ば | び | ぶ | べ | ぼ | . h | は | ひ | ふ (fu) | へ | ほ | . p | ぱ | ぴ | ぷ | ぺ | ぽ | . Observations . The sounds that are produced after adding a dakuten to k, s, and t make logical sense: k, s, and t are similar to g, z, and d respectively. . However, adding a dakuten to h to produce the b consonant and adding a handukuten to h to produce the p sound both do not make too much sense. It would be interesting to see why h becomes b and p. . Transcribing Contracted Sounds . . A further 21 syllables are derived by contracting certain syllables with a small や, ゆ, and よ. .   | ya | yu | yo | . k | きゃ | きゅ | きょ | . s | しゃ (sha) | しゅ (shu) | しょ (sho) | . t | ちゃ (cha) | ちゅ (chu) | ちょ (cho) | . n | にゃ | にゅ | にょ | . h | ひゃ | ひゅ | ひょ | . m | みゃ | みゅ | みょ | . r | りゃ | りゅ | りょ | . g | ぎゃ | ぎゅ | ぎょ | . z | じゃ (ja) | じゅ (ju) | じょ (jo) | . b | びゃ | びゅ | びょ | . p | ぴゃ | ぴゅ | ぴょ | . い vowel syllables are used for contraction. . Transcribing Double Consonants . . Small つ is used for transcribing double consonants. . かった — katta (won) かた — kata (shoulder) . っ is not used to transcribe double consonant n sounds. Instead, ん is used along with a hiragana that has an initial n sound. . さんねん — sannen あんない — annai . Observations . The use of っ to transcribe double consonants is similar to Urdu. In Urdu, شدّ (shadda) is used: . ّ . Long Vowels . Same vowels that are placed consecutively next to each other are pronounced twice as long. . おばあさん — obaasan (grandmother) おばさん — obasan (aunt) . Long ee is typically transcribed by adding い instead of え. However, there are exceptions. . えいが — eega おねえさん — oneesan . Similarly, long oo is typically transcribed by adding う instead of お. Again, there are exceptions. . ほうりつ — hooritsu とお — too . Observations . There is similarity with Urdu again. In Urdu, و ا and ی are used for long vowels while َ ِ and ُ are respectively used to denote short vowels. . Pronunciation of ん . ん is pronounced nasally when it follows a vowel or completes an utterance. . れんあい — rẽnai ほん — hõ . Observations . This letter is similar to ں (noon ghunna — نون غنّہ) in Urdu, which is used to produce nasal n. . Accent . Japanese is a tonal language and generally has two tones: high and low pitch. Accent can vary depending on the region. . Dropped Vowels . i and u are sometimes dropped when placed between or at the end of voiceless consonants (those consonants which do not require your voicebox to work). Consonants in particular are k, s, t, p, and h. . すきです — s(u)kides(u) . Conclusion . Japanese does feel like a different language to me since it is a completely new writing system. It has more similarities to Urdu than English and has shown how difficult English is to learn as a language for nonnative speakers, just from the alphabet itself (uppercase and lowercase letters look completely different in many cases and vowels themselves produce different sounds; e.g., the letter “i” makes the sound eye, i and ee). .",
            "url": "https://forbo7.github.io/ForBlog/japanese/hiragana/2022/08/13/Hiragana.html",
            "relUrl": "/japanese/hiragana/2022/08/13/Hiragana.html",
            "date": " • Aug 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Quick Note: Switching from Chinese to Japanese",
            "content": ". As a quick note, I am now learning Japanese instead of Chinese. I will share why at a later date. In the meantime, I will now be sharing my journey learning Japanese! I will be following the Genki textbook series. .",
            "url": "https://forbo7.github.io/ForBlog/2022/08/11/Switching-from-Chinese-to-Japanese.html",
            "relUrl": "/2022/08/11/Switching-from-Chinese-to-Japanese.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Do You Have a Bīngxiāng?",
            "content": ". Takeaways from Lesson 4: Around the Home from the Living Language Essential Chinese book. . Observations . Prefixes and Suffixes . I infer that diàn (电) has to do with technology: . diànshì (电视) — television | diànhuà (电话) — telephone | diànnǎo (电脑) — computer | . I infer that chē (车) has to do with mobility: . qìchē (汽车) — car | zìxíngchē (自行车) — bicycle | . I also infer that zi (子) has something to do with furniture: . zhuōzi (桌子) — table | yǐzi (椅子) — chair | . My inference for diàn (电) was in the ballpark! As later on, the lesson said that diàn (电) has to with electricity. . Who and Whom . The word shéi (谁) is used for both who and whom. When used in question, shéi (谁) negates the use of ma (吗). When responding, replace shéi (谁) with the appropriate pronoun. . Shéi yǒu zhuōzi? （谁有桌子？） . Tā yǒu yī zhāng zhuōzi. （他有一张桌子。） . Shéi yǒu diànnǎo? （谁有电脑？） . Tāmen yǒu diànnǎo. （他们有电脑。） . And . ‘and’ (hé/和) is frequently omitted in Mandarin. A comma is used in its place. . I have three computers and two cars. . Wǒ yǒu sān tái diànnǎo, liǎng liàng qìchē. (我有三台电脑，两辆汽车。) . More Turning Strokes . Héngzhéwāngōu . The héngzhéwāngōu turning stroke looks like ح in Urdu. . 乙 . Héngzhézhézhégōu . This turning stroke is a héngzhé followed by a héngzhégōu. . 𠄎 . Shùzhézhégōu . It is a shùzhé followed by a shùgōu. . ㇉ . Héngpiě . This is simply, as stated in by its name, a héng followed by a piě. . ㇇ . Héngpiěwāngōu . The héngpiěwāngōu, as also stated by its name, is a héngpiě followed by a wāngōu. . ㇌ . This turning stroke is also known as the guàěr stroke because it resembles an ear. . The meaning of zhé . Theses turning strokes include a lot of “zhé” in their name. I infer that zhé has something to do with turns made in the stroke itself. . More Measure Words . běn . This measure word is essentially used for books (the book states books, photo albums, and magazines). . Wǒ yǒu sān běn shū. (我有三本书。) . liàng . This measure word is essentially used for vehicles (the book states cars, taxis, and bicycles.) . Xiānsheng yǒu liǎng liàng qìchē. （先生有两辆汽车。） . Wǒmen yǒu sì liàng zìxíngchē. (我们有四辆自行车。) . tái . This measure is used for machines (e.g., computers, televisions, sewing machines, air conditioners). . Nánhái yǒu yī tái diànshì. (男孩有一台电视。) . Jiějie yǒu yī tái diànhuà. (姐姐有一台电话。) . zhāng . This measure word is essentially used for furniture (the book states tables, desks, and chairs.) . Lǎoshī yǒu èrshí’èr zhāng yǐzi, shíyī zhāng zhuōzi. (老师有二十二张椅子，十一张桌子。) . Conclusion . That concludes my observations from this lesson. Up next: descriptions, adjectives, and colors. . And since you’ve made it to the end, “bīngxiāng” is “refrigerator” in Mandarin! . If you have any comments, suggestions, or corrections, please do post them down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/mandarin/hanz%C3%AC/2022/06/09/Do-You-Have-A-B%C4%ABngxi%C4%81ng.html",
            "relUrl": "/mandarin/hanz%C3%AC/2022/06/09/Do-You-Have-A-B%C4%ABngxi%C4%81ng.html",
            "date": " • Jun 9, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "📌 Data Quality is Important | Car Classifier",
            "content": ". I recently created a car classifier that classified cars into their brand. . Despite having almost 5000 images in training set, I ended up having to use over a hundred layers (I ended up using ResNet101) in my model and twenty epochs. Even then, I still had an error rate of 17.4%. . The culprit? My dataset. . I scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good. . Why? Below are some potential reasons: . Noncar images present in dataset | Cars of incorrect company present in dataset | F1 cars present in dataset | A large variety of cars from different time periods present in dataset | Different companys’ cars look similar | Modded cars present in dataset | Concept cars present in dataset | Multiple cars present in a single image | Certain angles of cars appear more than others | Cars appear in certain backgrounds more than others | The search term {car_brand} car could be skewing results | . I could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted. . Below is the confusion matrix for this model. . . It can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford. . But why is data quality important? Because without good data, the model will not be able to “see” things the way they actually are and in turn, end up making worse predictions and not be able to generalize to other data. . Let’s say you did not know how a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you find the toaster. You would not be able to. . Similarly, if instead I showed you toasters only from two brands and toasters from only the last two years, you would not be able to idenfity toasters from other brands or from other years. . Obviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical. . The point of this post is to emphasize the importance of quality data and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world but it is pretty useless if you don’t have good data. . If you have any comments, questions, suggestions, or corrections, please do post them down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/data/data%20cleaning/analyzing%20models/2022/06/04/Data-Quality-Is-Important.html",
            "relUrl": "/data/data%20cleaning/analyzing%20models/2022/06/04/Data-Quality-Is-Important.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "A No Nonsense Guide to Reading a Confusion Matrix",
            "content": ". Confusion matrices help us model designers view what mistakes our model has made. . I’ll be approaching this with logical examples. . Jump to Case 2 for an ultra concise rundown. . Ready? Here we go. . Case 1: Introduction . . Ignore the “Actual” and “Predicted” labels for now. . Let’s compare grizzly bears to black bears. . All comparisons begin at the bottom, and with the columns. . First, highlight the grizzly bear column. . . Next, highlight the black bear row. . . Now find the common entry in the highlighted column and row. . . This common entry is our required information. . All entries in the main diagonal are correct classifications. All other entries are incorrect classifications. . Our common entry does not lay in the main diagonal. Therefore we are looking at incorrect classifications. . We have compared grizzly bears to black bears. Therefore, from this deduction, three grizzly bears have been incorrectly classified as black bears. . . There is a difference between comparing grizzly bears to black bears and black bears to grizzly bears. Comparing grizzly bears to black bears means, &#39;How many grizzly bears were misclassified as black bears?&#39; Comparing black bears to grizzly bears means, &#39;How many black bears were misclassified as grizzly bears?&#39; Case 2: Ultra Concise . Let’s compare black bears to grizzly bears. . Highlight the black bear column. . . Highlight the grizzly bear row. . . Highlight the common entry. . . Zero grizzly bears were misclassified as black bears. . Case 3: Correct Classifications . Let’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears. . Highlight the teddy bear column. . . Highlight the teddy bear row. . . Highlight the common entry. . . Fifty three teddy bears were correctly classified as teddy bears. . Exercise: Do It Yourself . Below is a confusion matrix of a car classifier that classifies cars into their brand. . . You learn by doing! . How many Lamborghinis were correctly classified? | How many Jaguars were incorrectly classified? | How many Chevrolets were misclassified as Fords? | How many Fords were misclassified as Chevrolets? | Which two car brands did the model have the most trouble differentiating between? | . If you have any questions, comments, suggestions, or corrections, please do post them down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/how%20to/guide/confusion%20matrix/analyzing%20models/2022/06/03/The-Confusion-Matrix.html",
            "relUrl": "/how%20to/guide/confusion%20matrix/analyzing%20models/2022/06/03/The-Confusion-Matrix.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "四， 五， 六 | Counting in Mandarin",
            "content": ". Introduction . Numbers are easy in Mandarin, perhaps even easier than English. . Below are observations from Lesson 3: Numbers of the Living Language Essential Chinese book. . Observations . Zero to Ten . líng | yī | èr | sān | sì | wǔ | liù | qī | bā | jiǔ | shí | . But what is easy about this? . Eleven to Nineteen . To say the teens, you say ten then the appropriate number. So instead of saying twelve, you would say ten two. . shíyī | shí’èr | shísān | shísì | shíwǔ | shíliù | shíqī | shíbā | shíjiǔ | . Twenty to Ninety . See how easy it is? Saying the tens is similar. You first say ten then the appropriate number. Instead of saying twenty, you say two ten. . èrshí | sānshí | sìshí | wǔshí | liùshí | qīshí | bāshí | jiǔshí | . Hundred to Million . The logic previously applied works similarly for hundred to million. . Hundred is bǎi | Thousand is qiān | Ten thousand is wàn or yīwàn | . Therefore, . One hundred is yībǎi | Two thousand is èrqiān | Thirty thousand is sānwàn | . Wàn is treated as its own unit, similar to hundred, thousand or million. It would be ten thousand in English by one wàn in Mandarin. . Wàn is also similar to lakh in Urdu. One lakh in one hundred thousand in English. . Turning Strokes . Hanzì also contains turning strokes; that is, strokes that contain turns and are written in one continuous flow (ignore that fact that the images for the turning strokes below look like they are composed of multiple strokes). . Below are five. . Héngzhé It is a héng with a shù. . . Image credit 1 . Héngzhégōu . It is a héngzhé with a gōu. . . Image credit 2 . Shùzhé . It is a shù with a héng. . . This turning stoke can either have a short shù with a long héng or a long shù with a short héng. . Image credit 3 . Piězhé . It is a piě with a héng. . . The angle between the two can differ at times. . Image credit 4 . Shùwāngōu . It’s a shù with a wāngāou rotated 90 degrees. Easier to remember it as a whole new stroke than it being a composed of individual strokes. . . Image credit 5 . Measure Words . Mandarin uses measure words when dealing with quantities. Measure words are similar to “pair” in “one pair of shoes” and “glass” in “five glasses of water”. . The most common measure word is gè and is essentially used for people and places (the book states people, cities, groups, and nations). . This is how you would state the following in Mandarin: . He has three students. . Tā yǒu sān gè xuésheng. . I have one younger brother. . Wǒ yǒu yī gè dìdi. . Dual Quantities . When dealing with quantities that have two of something, liǎng is used in place of èr. . You would not say: . Nǐmen yǒu èr gè lǎoshī. . Instead, you would say: . Nǐmen yǒu liǎng gè lǎoshī. . Plurals . As I suspected previously in this post, there are no plurals in Mandarin. . Conclusion . That completes my observations for lesson 3! The next post will be about objects around the house, which includes a few more measure words. . If you have any comments, suggestions, or corrections, please do post them down in the comment section below! . Acknowledgements . Cjk m str hv by Canjie6 / CC BY 4.0 &#8617; . | Cjk m str hvj by Canjie6 / CC BY 4.0 &#8617; . | Cjk m str vh by Canjie6 / CC BY 4.0 &#8617; . | Cjk m str tu by Canjie6 / CC BY 4.0 &#8617; . | Cjk m str vaj by Canjie6 / CC BY 4.0 &#8617; . |",
            "url": "https://forbo7.github.io/ForBlog/mandarin/numbers/hanz%C3%AC/2022/06/02/%E5%9B%9B-%E4%BA%94-%E5%85%AD.html",
            "relUrl": "/mandarin/numbers/hanz%C3%AC/2022/06/02/%E5%9B%9B-%E4%BA%94-%E5%85%AD.html",
            "date": " • Jun 2, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "📌 Bear Classifier",
            "content": ". Introduction . Below is my first attempt at creating my own model, with the help from the fastai course. This is an image classification model which can tell you whether a bear in an image is a grizzly bear, black bear, or teddy bear. . You can visit the classifier here to test it out for yourself! Note that it may take a minute or two for the site to load. . Load libraries . !pip install fastbook # No need to fret! fastai is specifically designed to be used with *. from fastbook import * from fastai import * from fastai.vision.all import * from fastai.vision.widgets import * . Download image files. . Specify the bears we wish to download. . bear_types = (&#39;grizzly&#39;, &#39;black&#39;, &#39;teddy&#39;,) path = Path(&#39;bears&#39;) . Download 200 (search_images_ddg defaults to 200 URLs) of each bear and create and assign them to a specific directory. . if not path.exists(): path.mkdir() for bear_type in bear_types: dest = (path/bear_type) dest.mkdir(exist_ok=True) urls = search_images_ddg(f&quot;{bear_type} bear&quot;) download_iamges(dest, urls=urls) . Check if our folder has the image files. . fns = get_image_files(path) fns . (#802) [Path(&#39;bears/grizzly/00000238.jpg&#39;),Path(&#39;bears/grizzly/00000047.jpg&#39;),Path(&#39;bears/grizzly/00000199.jpg&#39;),Path(&#39;bears/grizzly/00000237.jpg&#39;),Path(&#39;bears/grizzly/00000055.jpg&#39;),Path(&#39;bears/grizzly/00000000.png&#39;),Path(&#39;bears/grizzly/00000235.jpg&#39;),Path(&#39;bears/grizzly/00000159.jpg&#39;),Path(&#39;bears/grizzly/00000268.jpg&#39;),Path(&#39;bears/grizzly/00000266.jpg&#39;)...] . Check for corrupt images. . corrupt_images = verify_images(fns) corrupt_images . (#0) [] . Remove corrupt images. . corrupt_images.map(pathlib.Path.unlink) . (#0) [] . Load image files . DataBlock API . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128), ) . The blocks parameter allows us to specify the independent and dependent variables. . The get_items parameters tells fastai how to obtain our data. We use the get_image_files function to obtain our images. . The splitter parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the RandomSplitter class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time. . The get_y parameter obtains our labels. The parent_label function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, with each folder having a different bear type, this will nicely handle our target labels. . The item_tfms parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the Resize() class. . We now have a DataBlock object from which can load the data. . dls = bears.dataloaders(path) . Let us view a few images in the validation set. . dls.valid.show_batch(max_n=4, nrows=1) . Data Augmentation . Data augmentation refers to creating random variations of our input data so that they look different, but do not change their meaning. . Typical examples of data augmentation relating to images include rotation, flipping, perspective warping, brightness changes, and contrast changes. . Cropping . Above in the images of the validation set, we applied the Resize augmentation function. This function crops images to the size specified, which results in detail being lost. . Alternatively, we can squish/stretch images or pad them to our desired size. . Squishing/Stretching . The problem with squishing or stretching images to the same size is that th emodel will learn to &quot;see&quot; images the way they are not actually are. . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . Padding . The problem with padding images is that it adds extra wasted computation (the black pixels). . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . The best approach is to take random crops of different parts of the same image so that the neural network does not miss out on any details and can understand that an object can appear in different orientations. . Below, we have unique=True so that the same image is repeated with different variations. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . fastai comes with a function that applies a variety of augmentations to images. This can allow a model to &quot;see&quot; and recognize images in a variety of scenarios. . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Note that RandomResizedCrop is not being used so that differences can be seen more clearly. We are using the batch_tfms parameter to tell fastai that we want to use these transforms on a batch. . Training the mdoel . We do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations. . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms(), ) dls = bears.dataloaders(path) . We will now create our learner and fine-tune it. . We will be using the ResNet18 architecture (which is a convolutional neural network or CNN for short) and error rate as the metric. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 0.985666 | 0.104632 | 0.025000 | 00:20 | . epoch train_loss valid_loss error_rate time . 0 | 0.132230 | 0.073527 | 0.012500 | 00:22 | . 1 | 0.106222 | 0.054833 | 0.018750 | 00:22 | . 2 | 0.087129 | 0.058497 | 0.012500 | 00:20 | . 3 | 0.069890 | 0.058845 | 0.018750 | 00:19 | . Our model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs&#39; error rates. . Visualizing mistakes . We can visualize the mistakes the model is making by a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . 3 grizzly bears were misclassified as black bears. . Let us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem. . To do this, we will sort images by their loss. . interp.plot_top_losses(5, nrows=1) . Data cleaning . The intuitive approach to data cleaning is to do it before training the model. However, a trained model can also help us clean the data. For example, we could see some mislabaled bears in the above cases. . fastai includes a GUI that allows you to choose a category/label and their associated training and validation sets and view the highest-loss images in order so that you can select images for removal or relabeling. . cleaner = ImageClassifierCleaner(learn) cleaner . ImageClassifierCleaner does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled. . for index in cleaner.delete(): cleaner.fns[index].unlink() # Relabel images selected for relabeling. for index, category in cleaner.change(): shutil.move(str(cleaner.fns[index]), path/category) . We can now retrain and a better performance should be expected. . Saving the model . A model consists of two parts: the architecture and the parameters. . When we use the export() method, both of these are saved. . This method also saves the definition of your DataLoaders. This is done so that you do not have to redefine how to transform your data to use your model in production. . fastai uses your validation set DataLoader by default, so the data augmentation will not be applied, which is generally what you want. . The export() method creates a file named &quot;export.pkl&quot;. . learn.export() . Let us check that the file exists. . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . If you wish to deploy an app, this is the file you will need. . Loading the model for inference . Now obviously we do not need to load the model as we already have the learner variable. However, I will do so here for the sake of practice lol. . learn_inf = load_learner(path/&#39;export.pkl&#39;) . We generally do inference for a single image at a time. . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08])) . Three things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category. . The order of each category is based of the order of the vocab of the DataLoaders; that is, the stored tuple of all possible categories. . The DataLoaders can be accessed as an attribute of the Learner. . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . A word about CNNs . Since the ResNet18 architecture is a sort of CNN, I will explain why CNNs work so well for images. I do not have a full understanding, but this is what I understand so far. . Essentially, each neuron in a layer is given the exact same weights and all neurons are input different data. This is so that all neurons fire upon detecting the same patterns. . By all neurons, in a layer, being given the exact same weights, it allows those neurons to all be triggered by the same pattern. This is why CNNs are really good at detecting objects in various patterns, orientations, shapes, positions, and so on. . Conclusion . Well then, that wraps up my first deep learning model! I have to say, it is much easier than I thought to implement a model. You do not need to go into the nitty gritty details in artificial intelligence to be able to implement them. A high level understanding will suffice. It is like playing a sport: you do not need to understand the physics to be able to play it. . Right now I am creating my own classifier that will classify cars into their company types. . If you have any comments, suggestions, or corrections, please do let me know down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/fastai/image%20classification/cnn/2022/05/28/bear_classifier_model.html",
            "relUrl": "/fastai/image%20classification/cnn/2022/05/28/bear_classifier_model.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "How to Approach Creating AI Models",
            "content": ". Introduction . How you approach making your model is crucial. Articial Intelligence is not about creating models; it is only a tiny part of it. It is 80% problem solving and 20% implementing (I would not be suprised if it actually followed the 80-20 rule1). . It is like the scientific method in a sense. Sure, you can create a really accurate, well functioning experiment. But what use is it if you do not approach conducting the experiment and analyzing the results in a well defined manner? . One highly successful approach is the Drivetrain Approach, created by fast.ai course creater Jeremy Howard along with his collegues Margit Zwemer and Mike Loukides. Their official blogpost on this approach goes into much detail and can be read in full here. The goal of this approach is to not use data just to generate more data (that is in the form of predictions), but to use data to also generate actionable outcomes. . Over here, I’ll be providing a short overview of my understanding of this approach (from the fast.ai course) and will be applying it to the final project I did in the University of Helsinki’s and Reaktor’s Elements of AI course (I highly highly recommend this course; it gives a really good primer into AI). . Overview of the Drivetrain Approach . There are four main steps to this process: . Define the objective | Consider your possible actions | Consider your data | Create the models | . Define the objective . Write out what you are really trying to achieve. What is your goal? . Consider your actions . Think about what actions you can take to achieve your objective. . Also think about what would happen if you did those actions. What would happen if did x? Would y really be a good idea? What if z worked really well? What if z worked really bad? . Consider your data . Think about the data you already have and how it can be used. Think about any further data that is needed and how it could be collected. . Create the model . Created models that produce the best actions that in turn produce the best results in terms of your objective. . Endangered Language Chatbot . The final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would. I ended up creating an overview for how a chatbot could be created to preserve endangered languages. . Now that I think of it, what the project asked me to do is one sort of approach to creating AI methods, though more on the why side. You can view my project overview here. . Define the objective . The objective of creating such a chatbot is to preserve languages that are endangered or extinct. This would help preserve different histories and cultures, as well as humanity’s diversity. . Consider your actions . Probably, a new sort of NLP architecture would have to be created primarily due to the lack of data there would be for these languages. . Alternatively, existing methods could be applied, but there would be varying degrees of success depending on how much data is available for a language. . Consider your data . The main source of data for training such models would corpuses of text. However, this itself is a problem since many endangered or extinct languages do not have enough written text that would achieve the level of performance required to have somewhat of a good conversation with the bot. Further troubles arise for those languages that are speech only; you would need to have audio recordings, which would be extremely difficult for a language that is endangered or nigh impossible for those that are extinct. . The main solution to these problems that comes to mind is literal manpower and brute force. That is, humans would have to manually create masses of text or speech that the model can then be trained on to “learn” the langauge. . Create the model . Create a model that speaks as accurately as a native of the endangered language so that any person interacting with the chatbot gets a vivid idea of the language, its culture, and its people. . Conclusion . This concludes my understanding and explanation of one such approach to creating models, along with an attempted example. Approaches are crucial: you can have all the best tools and the most accurate models in the world, but they are worthless if not correctly approached, used, and applied. . If you have any comments, suggestions, or corrections, please do post them down in the comment section below! . Footnotes . The 80/20 Rule, also known as the Pareto Principle &#8617; . |",
            "url": "https://forbo7.github.io/ForBlog/model%20deployment/drivetrain%20approach/2022/05/27/How-to-Approach-Creating-AI-Models.html",
            "relUrl": "/model%20deployment/drivetrain%20approach/2022/05/27/How-to-Approach-Creating-AI-Models.html",
            "date": " • May 27, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Mandarin: People and Family",
            "content": ". Nǐ shì rén ma? . And so I have completed Lesson 2 of Living Language’s Essential Chinese: People and Family. In this lesson, I learned about how certain people are referred to as well as certain grammatical insights. . I have to say, this chapter was particularly interesting because I saw how the Chinese think, in terms of language. I have studied English Language in high school, including how the language developed over the centuries, how children pick up langauge, how English has spread globally, and language and the self. . Observations . Male and Female . Indicating if a person is male or female is generally straightforward: you prefix nǔ (female) or nán (male) — yes, yes, this probably does not cover all cases. . For example, person in Mandarin is rén. Therefore, woman is nǔrén (female person) and man is nánrén (male person). . Boy is nánhái and girl is nǔhái. I infer that hái must mean something in the bout of a child. . There is also no verbal difference when referring to a male or female. He/him/she/her is all spoken using the same term: tā. However, there is a written difference. He/him is 他 and she/her is 她. . The Chinese then perhaps do not make a distinction between males and females, and perhaps rather let the context of the situation speak for itself. . Conjugates and Articles . Simply put, conjugates are terms such as “to be”, “is”, “are”, “was”, and “will be”. In Chinese, there is none of that nonsense. You use a single term for every thing, person, and all times including past, present, and future. That term is shì (是). . Articles are terms such as “a”, “an”, and “the”. Similarly, Chinese has none of that nonsense and uses no such terms. . The Chinese then probably do not have strict concepts of time, as is seen by the use of shì for past, present, and future. They perhaps think of time as one continuous flow. This is also seen by the term yǒu, which is used for have had, to have, and will have. . Plurals . To make any pronoun plural, you simply append -men (们) to the end. . nǐ (你）—&gt; nǐmen (你们) | wǒ (我) —&gt; wǒmen (我们) | tā (他) —&gt; tāmen (他们) | . I have noticed that there is perhaps no distinction between plural and singular nouns. Take the below sentences as an example. Xuésheng means student. . They are a student. (Tāmen shì xuésheng.) . They are our students. (Tāmen shì wǒmen xuésheng.) . Xuésheng is used for both student and students. . Family . The terms used to refer to siblings are very easy to pronounce. Perhaps so that young children can easily speak them, as is seen below: . younger brother: dìdi | older brother: gēge | younger sister: mèimei | older sister: jiějie | . More Strokes . Tí . As it sounds, goes from bottom left to top right. . ㇀ . It can be longer or shorter than what is shown above and can also be more horizontal/shallow too. . Shùgōu . It is a shù with with a gōu. . 亅 . Wāngōu . It is a shùgōu with an initial curve. . ㇁ . Shùtí1 . It is a shù with a tí. . . Xiégōu . It is like a curved shùtí. . ㇂ . Nà . It is like a mirrored piě. . ㇏ . It can be more horizontal or vertical than what is shown above. . Hanzì Characters . Hanzì characters can never contain more than one nà stroke. . Conclusion . That concludes my observation for Lesson 2. If you have any comments, suggestions, or corrections, please do put them down in the comments section below! . Next up is Lesson 3: Numbers! Wǒ yǒu yī gè dìdi. . Acknowledements . Shùtí stroke image: CJK stroke by Canjie6 / CC BY 4.0 &#8617; . |",
            "url": "https://forbo7.github.io/ForBlog/mandarin/hanz%C3%AC/2022/05/26/Mandarin-People-and-Family.html",
            "relUrl": "/mandarin/hanz%C3%AC/2022/05/26/Mandarin-People-and-Family.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Nǐ hǎo!",
            "content": ". Nǐ hǎo ma? . A fitting title for my first blog post! . So begins my venture into learning Mandarin! I am actually excited to begin learning a new language, partly because I also had a full physical course laying about. . The course in question is Living Language Chinese Complete Edition. It comes with four books (essential, intermediate, advanced, and a character guide) along with six audio CDs. Reviews for the Chinese course from Living Language seemed good too, though people tended to praise the older version more which is the one I have (I think). . . Let’s move on to what I have observed so far in learning this language, from Lesson 1 of the course. . Observations . Greetings . Nǐ hǎo. | Nǐ hǎo ma? | Nǐ ne? | . Nǐ must refer to the other person (you). . Wǒ hěn hǎo. | hěn hǎo | . Hǎo must refer to the good state [of someone]. . Tones . There are five tones in Mandarin. You can tell how to pronounce each tone by simply looking at its shape. . First tone . ā . A flat, high pitched tone. . Second tone . á . A rising tone, from medium to high pitch. . Third tone . ǎ . A scooping tone, from low to medium pitch. . Fourth tone . à . A falling tone, similar to when you say “yes” or “no” in response to a question, or when you saying something you really mean. . Fifth tone . a . A neutral, medium pitched tone. . Asking for help . Qǐng ràng yī xià? | Dǎrǎo yī xià? | Qǐngwèn yī xià? | . “yī xià” is probably used for those questions where you ask somebody for a favor or for an action from them. . “ma”, on the other hand, is probably used to turn a phrase into a question. . Hanzì strokes . Observations from Character Guide book, Lesson 1 — Basic Strokes. . Héng . As it sounds, is a horizontal line from left to right. . 一 . Shù . As it sounds, is a vertical line from top to bottom. . 丨 . Piě . Is like a curved shù that can be shallow or deep. The deep variant is shown below. . 丿 . Diǎn . Is sort of like a diagonal dot. Can be drawn from top left to bottom right or top right to bottom left. Essentially can only be drawn starting from the top. . 丶 . Zhé . Is like héng, but has a hook at the end. . 乛 . “hook” in pīnyīn is gōu. Therefore, zhé is also known as hénggōu. . Hanzì Characters . I have noticed that a single hanzì character corresponds to a single syllable. . Conclusion . Well then, that concludes my initial observation and things learned in Mandarin. I’ll be making more of these sorts of posts as I continue my learning journey. . I would appreciate any comments, corrections, and suggesstions! .",
            "url": "https://forbo7.github.io/ForBlog/mandarin/hanz%C3%AC/2022/05/21/N%C7%90-h%C7%8Eo.html",
            "relUrl": "/mandarin/hanz%C3%AC/2022/05/21/N%C7%90-h%C7%8Eo.html",
            "date": " • May 21, 2022"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "Artstation",
          "content": "Check out my ArtStation where I upload my projects and works relating to 3D computer graphics. .",
          "url": "https://forbo7.github.io/ForBlog/Artstation.html",
          "relUrl": "/Artstation.html",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "GitHub",
          "content": "Check out my GitHub where you can see various coding projects that I have dabbled including game development and my ventures into AI. .",
          "url": "https://forbo7.github.io/ForBlog/GitHub.html",
          "relUrl": "/GitHub.html",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About Me",
          "content": "I’m ForBo7: an approachable, adaptable, open-minded, curious individual who’s been to seventeen countries and lived in three. I’m a science and technology geek, space nerd, and currently dabble in AI. 3D computer graphics is my hobby and learning is my passion. .",
          "url": "https://forbo7.github.io/ForBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://forbo7.github.io/ForBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}