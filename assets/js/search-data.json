{
  
    
        "post0": {
            "title": "📌 Detecting Floods for Disaster Relief",
            "content": "You can find this notebook on Kaggle here. . . The model that will be created in this notebook can detect whether an area shown in an image is flooded or not. The idea for creating this model has been spurred from the recent floodings in Pakistan. . Such models can prove useful in flood relief, helping to detect which areas need immediate focus. . The dataset used to train this model is Louisiana flood 2016, uploaded by Kaggle user Rahul T P, which you can view here. . The fastai library, a high level PyTorch library, has been used. . One of the points of this notebook is to showcase how simple it is to create powerful models. That said, this notebook is not a tutorial or guide. . from fastai.vision.all import * . Sort data. . The data in the dataset needs to be organized into train and valid folders. Each will contain the same subfolders, 0 and 1, which will be used to label the data. A label of 0 indicates the area shown in the image is not flooded, while a label of 1 indicates the area shown in the image is flooded. . The images in the dataset itself has been organized as follows: . &nbsp;&nbsp;&nbsp;&nbsp;If no underscore is in the file name, the image shows the area before or after the flood. . &nbsp;&nbsp;&nbsp;&nbsp;If an underscore is in the file name, the image shows the area during the flood: . If a zero follows the underscore, the area was not flooded. | If a one follows the underscore, the area was flooded. | . Creating the necessary paths. . working_path = Path.cwd(); print(working_path) folders = (&#39;train&#39;, &#39;valid&#39;) labels = (&#39;0&#39;, &#39;1&#39;) . /kaggle/working . input_path = Path(&#39;/kaggle/input&#39;) train_image_paths = sorted(input_path.rglob(&#39;train/*.png&#39;)) valid_image_paths = sorted(input_path.rglob(&#39;test/*.png&#39;)) len(train_image_paths), len(valid_image_paths) . (270, 52) . Creating the necessary directories. . for folder in folders: if not (working_path/folder).exists(): (working_path/folder).mkdir() for label in labels: if not (working_path/folder/label).exists(): (working_path/folder/label).mkdir() . Move images to new directories. . try: for image_path in train_image_paths: if &#39;_1&#39; in image_path.stem: with (working_path/&#39;train&#39;/&#39;1&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) else: with (working_path/&#39;train&#39;/&#39;0&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) except FileExistsError: print(&quot;Training images have already been moved.&quot;) else: print(&quot;Training images moved.&quot;) . Training images moved. . try: for image_path in valid_image_paths: if &#39;_1&#39; in image_path.stem: with (working_path/&#39;valid&#39;/&#39;1&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) else: with (working_path/&#39;valid&#39;/&#39;0&#39;/image_path.name).open(mode=&#39;xb&#39;) as f: f.write(image_path.read_bytes()) except FileExistsError: print(&quot;Testing images have already been moved.&quot;) else: print(&quot;Testing images moved.&quot;) . Testing images moved. . Check that images have been moved. . training_images = get_image_files(working_path/&#39;train&#39;); print(len(training_images)) . 270 . Image.open(training_images[0]) . validation_images = get_image_files(working_path/&#39;valid&#39;); print(len(validation_images)) . 52 . Image.open(validation_images[-1]) . Load data . Create the training and validation dataloaders through fastai&#39;s quick and easy DataBlock class. . dataloaders = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = GrandparentSplitter(), get_y = parent_label, item_tfms = [Resize(192, method=&#39;squish&#39;)] ).dataloaders(working_path, bs=32) . Check that data has been loaded correctly. . dataloaders.show_batch(max_n=8) . Instantiate and Train Model . learner = vision_learner(dataloaders, resnet18, metrics=error_rate) learner.fine_tune(9) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 0.919323 | 1.118264 | 0.365385 | 00:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.490039 | 0.628054 | 0.250000 | 00:02 | . 1 | 0.367996 | 0.411558 | 0.192308 | 00:02 | . 2 | 0.266664 | 0.472146 | 0.192308 | 00:02 | . 3 | 0.203069 | 0.256436 | 0.115385 | 00:03 | . 4 | 0.158453 | 0.127106 | 0.076923 | 00:03 | . 5 | 0.124499 | 0.095927 | 0.038462 | 00:02 | . 6 | 0.098409 | 0.089279 | 0.038462 | 00:03 | . 7 | 0.079600 | 0.093277 | 0.038462 | 00:02 | . 8 | 0.064886 | 0.090372 | 0.038462 | 00:02 | . Nice! A relatively low error rate for no tweaking. . Visualizing Mistakes . We have to see how the model is getting confuzzled. . interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix() . Only a couple of mistakes. Let&#39;s see what they are. . interp.plot_top_losses(5, nrows=1) . Nothing has been mislabeled, but the first one is especially tricky to determine, even for human eyes. . Model Inference . Let&#39;s test the model on some images of the recent flooding in Pakistan. . def infer_image(image_path): display(Image.open(image_path)) label, _, probabilities = learner.predict(PILImage(PILImage.create(image_path))) if label == &#39;0&#39;: print(f&quot;The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.&quot;) elif label == &#39;1&#39;: print(f&quot;The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.&quot;) else: print(&quot;Unknown label assigned to image.&quot;) . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;1.jpeg&#39;) . The area shown in the image is not flooded with probability 65.65%. . Not bad! . Let&#39;s try it on another image. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;2.jpg&#39;) . The area shown in the image is flooded with probability 99.90%. . The label for this image is kind of meaningless. This is an image of a vast area of land, so certain areas could be flooded, while others are not. That said, it could be used to determine whether there is flooding in the image. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;3.jpg&#39;) . The area shown in the image is flooded with probability 99.99%. . The model performed really well in this case: the input image is shown at a different angle. The images in the training set only show areas from a top-down view. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;) . The area shown in the image is not flooded with probability 64.56%. . Over here, the limitations of the current state of the model can be seen. The model is not performing well on images where the view is more parallel to the ground, since the images in the training set are all top-down. . Let&#39;s do two more images. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;5.jpg&#39;) . The area shown in the image is flooded with probability 99.94%. . infer_image(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;6.jpg&#39;) . The area shown in the image is flooded with probability 100.00%. . The model is working well with images of different sizes too, and has given this image a very high, correct confidence. . Improving the model. . Let&#39;s see if we can get the model&#39;s performance to improve on the image the following image through augmenting the training set. . Image.open(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;) . augmented_dataloaders = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = GrandparentSplitter(), get_y = parent_label, item_tfms = RandomResizedCrop(192, min_scale=0.5), batch_tfms=aug_transforms() ).dataloaders(working_path, bs=32) . augmented_dataloaders.show_batch(max_n=8) . augmented_learner = vision_learner(augmented_dataloaders, resnet18, metrics=error_rate) augmented_learner.fine_tune(9) . epoch train_loss valid_loss error_rate time . 0 | 1.161182 | 0.835870 | 0.365385 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.442552 | 0.686252 | 0.288462 | 00:03 | . 1 | 0.417739 | 0.411907 | 0.153846 | 00:02 | . 2 | 0.346400 | 0.316388 | 0.057692 | 00:03 | . 3 | 0.306782 | 0.213407 | 0.076923 | 00:02 | . 4 | 0.251947 | 0.199586 | 0.076923 | 00:02 | . 5 | 0.209951 | 0.141818 | 0.057692 | 00:02 | . 6 | 0.188433 | 0.116713 | 0.057692 | 00:03 | . 7 | 0.169689 | 0.125078 | 0.057692 | 00:02 | . 8 | 0.151843 | 0.131188 | 0.057692 | 00:02 | . Let&#39;s try the new model out. . display(Image.open(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;)) label, _, probabilities = augmented_learner.predict(PILImage(PILImage.create(input_path/&#39;floodclassifiertestset&#39;/&#39;1&#39;/&#39;4.jpg&#39;))) if label == &#39;0&#39;: print(f&quot;The area shown in the image is not flooded with probability {probabilities[0]*100:.2f}%.&quot;) elif label == &#39;1&#39;: print(f&quot;The area shown in the image is flooded with probability {probabilities[1]*100:.2f}%.&quot;) else: print(&quot;Unknown label assigned to image.&quot;) . The area shown in the image is flooded with probability 99.91%. . Dang, impressive! The correct label and with excellent confidence! . Before we get too excited though, we should check the performance on the model with the previous images. . test_dataloader = learner.dls.test_dl([image_path for image_path in sorted((input_path/&#39;floodclassifiertestset&#39;).rglob(&#39;*.*&#39;))]) . probabilities, _, labels = augmented_learner.get_preds(dl=test_dataloader, with_decoded=True) . print(&quot;Images are numbered horizontally.&quot;) test_dataloader.show_batch() for probability, label, image_number in zip(probabilities, labels, range(1, 7)): if label == 1: print(f&quot;Image {image_number} is flooded with a probability of {probability[1]*100:.2f}%.&quot;) elif label == 0: print(f&quot;Image {image_number} is not flooded with a probability of {probability[0]*100:.2f}%.&quot;) else: print(f&quot;Image {image_number} has been assigned an unknown label.&quot;) . Images are numbered horizontally. Image 1 is flooded with a probability of 95.94%. Image 2 is flooded with a probability of 99.92%. Image 3 is flooded with a probability of 91.34%. Image 4 is flooded with a probability of 99.71%. Image 5 is flooded with a probability of 100.00%. Image 6 is flooded with a probability of 100.00%. . Drastically improved probabilities! A little augmentation can go a long way. . Takeaways . This model was trained on only 270 images and minimal code. Accessbility and abstraction to the field of machine learning has come a long, long way. Given the right data and the right pretrained model, a powerful model can be produced in less than an hour, if not half. . This is important: in disasters such as floods, the time taken to produce the logistics required for relief can be drastically reduced. It is also important because the barrier of entry to this field is dramatically lowered; more people can create powerful models, in turn producing better solutions. . However, there could be some improvements and additions made to the model: . Include a third class to the model. Images that are not flooded, but show signs of having been flooded would be assigned this class. The dataset used for this model includes such images. | Train the model on images that include a variety of geographic locations and dwellings. The current dataset only contains images taken in a lush, green area with plenty of trees; infrastructure looks a certain way; the color of the floodwater is also dependent on the surroundings. All this makes the model good a prediciting whether an image is flooded for images with certain features. | . If you have any comments, suggestions, feedback, criticism, or improvements, please do post them down in the comment section below!. .",
            "url": "https://forbo7.github.io/ForBlog/fastai/image%20classification/2022/09/12/Detecting-Floods-for-Disaster-Relief.html",
            "relUrl": "/fastai/image%20classification/2022/09/12/Detecting-Floods-for-Disaster-Relief.html",
            "date": " • Sep 12, 2022"
        }
        
    
  
    
  
    
  
    
  
    
  
    
  
    
        ,"post6": {
            "title": "📌 Data Quality is Important | Car Classifier",
            "content": ". I recently created a car classifier that classified cars into their brand. . Despite having almost 5000 images in training set, I ended up having to use over a hundred layers (I ended up using ResNet101) in my model and twenty epochs. Even then, I still had an error rate of 17.4%. . The culprit? My dataset. . I scraped 5000 images of cars (500 for each company) from DuckDuckGo. Naturally, as expected, the data quality is not so good. . Why? Below are some potential reasons: . Noncar images present in dataset | Cars of incorrect company present in dataset | F1 cars present in dataset | A large variety of cars from different time periods present in dataset | Different companys’ cars look similar | Modded cars present in dataset | Concept cars present in dataset | Multiple cars present in a single image | Certain angles of cars appear more than others | Cars appear in certain backgrounds more than others | The search term {car_brand} car could be skewing results | . I could have absolutely achieved better results with fewer layers and fewer epochs if I trained the model on better quality data — or manually combed through the 5000 images 💀. However, I did use fastai’s GUI for data cleaning. This GUI sorts images by their loss which helps to determine if certain images should be relabeled or deleted. . Below is the confusion matrix for this model. . . It can be seen that this model “confuses” between quite a few different brands: Ford and Chevrolet, Chevrolet and Ford, Jaguar and Aston Martin, Renault and Ford. . But why is data quality important? Because without good data, the model will not be able to “see” things the way they actually are and in turn, end up making worse predictions and not be able to generalize to other data. . Let’s say you did not know how a toaster looked like. So I taught you by showing you pictures of a kettle. Then to test you, I showed you a set of pictures depicting various kitchen appliances and told you find the toaster. You would not be able to. . Similarly, if instead I showed you toasters only from two brands and toasters from only the last two years, you would not be able to idenfity toasters from other brands or from other years. . Obviously, humans are smarter and can infer. AI methods can only infer to a certain degree, mainly based on what is in their dataset. This talk does start to become more philosophical. . The point of this post is to emphasize the importance of quality data and different aspects to consider as to why data quality may not be good. You can have the best architecture in the world but it is pretty useless if you don’t have good data. . If you have any comments, questions, suggestions, or corrections, please do post them down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/data/data%20cleaning/analyzing%20models/2022/06/04/Data-Quality-Is-Important.html",
            "relUrl": "/data/data%20cleaning/analyzing%20models/2022/06/04/Data-Quality-Is-Important.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "A No Nonsense Guide to Reading a Confusion Matrix",
            "content": ". Confusion matrices help us model designers view what mistakes our model has made. . I’ll be approaching this with logical examples. . Jump to Case 2 for an ultra concise rundown. . Ready? Here we go. . Case 1: Introduction . . Ignore the “Actual” and “Predicted” labels for now. . Let’s compare grizzly bears to black bears. . All comparisons begin at the bottom, and with the columns. . First, highlight the grizzly bear column. . . Next, highlight the black bear row. . . Now find the common entry in the highlighted column and row. . . This common entry is our required information. . All entries in the main diagonal are correct classifications. All other entries are incorrect classifications. . Our common entry does not lay in the main diagonal. Therefore we are looking at incorrect classifications. . We have compared grizzly bears to black bears. Therefore, from this deduction, three grizzly bears have been incorrectly classified as black bears. . . There is a difference between comparing grizzly bears to black bears and black bears to grizzly bears. Comparing grizzly bears to black bears means, &#39;How many grizzly bears were misclassified as black bears?&#39; Comparing black bears to grizzly bears means, &#39;How many black bears were misclassified as grizzly bears?&#39; Case 2: Ultra Concise . Let’s compare black bears to grizzly bears. . Highlight the black bear column. . . Highlight the grizzly bear row. . . Highlight the common entry. . . Zero grizzly bears were misclassified as black bears. . Case 3: Correct Classifications . Let’s see how many teddy bears were correctly classified. We are essentially comparing teddy bears to teddy bears. . Highlight the teddy bear column. . . Highlight the teddy bear row. . . Highlight the common entry. . . Fifty three teddy bears were correctly classified as teddy bears. . Exercise: Do It Yourself . Below is a confusion matrix of a car classifier that classifies cars into their brand. . . You learn by doing! . How many Lamborghinis were correctly classified? | How many Jaguars were incorrectly classified? | How many Chevrolets were misclassified as Fords? | How many Fords were misclassified as Chevrolets? | Which two car brands did the model have the most trouble differentiating between? | . If you have any questions, comments, suggestions, or corrections, please do post them down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/how%20to/guide/confusion%20matrix/analyzing%20models/2022/06/03/The-Confusion-Matrix.html",
            "relUrl": "/how%20to/guide/confusion%20matrix/analyzing%20models/2022/06/03/The-Confusion-Matrix.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
  
    
        ,"post9": {
            "title": "📌 Bear Classifier",
            "content": ". Introduction . Below is my first attempt at creating my own model, with the help from the fastai course. This is an image classification model which can tell you whether a bear in an image is a grizzly bear, black bear, or teddy bear. . You can visit the classifier here to test it out for yourself! Note that it may take a minute or two for the site to load. . Load libraries . !pip install fastbook # No need to fret! fastai is specifically designed to be used with *. from fastbook import * from fastai import * from fastai.vision.all import * from fastai.vision.widgets import * . Download image files. . Specify the bears we wish to download. . bear_types = (&#39;grizzly&#39;, &#39;black&#39;, &#39;teddy&#39;,) path = Path(&#39;bears&#39;) . Download 200 (search_images_ddg defaults to 200 URLs) of each bear and create and assign them to a specific directory. . if not path.exists(): path.mkdir() for bear_type in bear_types: dest = (path/bear_type) dest.mkdir(exist_ok=True) urls = search_images_ddg(f&quot;{bear_type} bear&quot;) download_iamges(dest, urls=urls) . Check if our folder has the image files. . fns = get_image_files(path) fns . (#802) [Path(&#39;bears/grizzly/00000238.jpg&#39;),Path(&#39;bears/grizzly/00000047.jpg&#39;),Path(&#39;bears/grizzly/00000199.jpg&#39;),Path(&#39;bears/grizzly/00000237.jpg&#39;),Path(&#39;bears/grizzly/00000055.jpg&#39;),Path(&#39;bears/grizzly/00000000.png&#39;),Path(&#39;bears/grizzly/00000235.jpg&#39;),Path(&#39;bears/grizzly/00000159.jpg&#39;),Path(&#39;bears/grizzly/00000268.jpg&#39;),Path(&#39;bears/grizzly/00000266.jpg&#39;)...] . Check for corrupt images. . corrupt_images = verify_images(fns) corrupt_images . (#0) [] . Remove corrupt images. . corrupt_images.map(pathlib.Path.unlink) . (#0) [] . Load image files . DataBlock API . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128), ) . The blocks parameter allows us to specify the independent and dependent variables. . The get_items parameters tells fastai how to obtain our data. We use the get_image_files function to obtain our images. . The splitter parameter allows us to tell fastai how to split our data into training and validation sets. Since our data is one big set, we use the RandomSplitter class and tell it to use 20% of our data as the validation set. We specify a seed so the same split occurs each time. . The get_y parameter obtains our labels. The parent_label function simply gets the name of the folder a file is in. Since we have organized our bear images into different folders, with each folder having a different bear type, this will nicely handle our target labels. . The item_tfms parameter allows us to specify a transform to apply to our data. Since we want all our images to be of the same size, we use the Resize() class. . We now have a DataBlock object from which can load the data. . dls = bears.dataloaders(path) . Let us view a few images in the validation set. . dls.valid.show_batch(max_n=4, nrows=1) . Data Augmentation . Data augmentation refers to creating random variations of our input data so that they look different, but do not change their meaning. . Typical examples of data augmentation relating to images include rotation, flipping, perspective warping, brightness changes, and contrast changes. . Cropping . Above in the images of the validation set, we applied the Resize augmentation function. This function crops images to the size specified, which results in detail being lost. . Alternatively, we can squish/stretch images or pad them to our desired size. . Squishing/Stretching . The problem with squishing or stretching images to the same size is that th emodel will learn to &quot;see&quot; images the way they are not actually are. . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . Padding . The problem with padding images is that it adds extra wasted computation (the black pixels). . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . The best approach is to take random crops of different parts of the same image so that the neural network does not miss out on any details and can understand that an object can appear in different orientations. . Below, we have unique=True so that the same image is repeated with different variations. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . fastai comes with a function that applies a variety of augmentations to images. This can allow a model to &quot;see&quot; and recognize images in a variety of scenarios. . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Note that RandomResizedCrop is not being used so that differences can be seen more clearly. We are using the batch_tfms parameter to tell fastai that we want to use these transforms on a batch. . Training the mdoel . We do not have a lot of data. Only 200 images of each bear at most. Therefore, we will augment our images not only to get more data, but so that the model can recognize data in a variety of situations. . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms(), ) dls = bears.dataloaders(path) . We will now create our learner and fine-tune it. . We will be using the ResNet18 architecture (which is a convolutional neural network or CNN for short) and error rate as the metric. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 0.985666 | 0.104632 | 0.025000 | 00:20 | . epoch train_loss valid_loss error_rate time . 0 | 0.132230 | 0.073527 | 0.012500 | 00:22 | . 1 | 0.106222 | 0.054833 | 0.018750 | 00:22 | . 2 | 0.087129 | 0.058497 | 0.012500 | 00:20 | . 3 | 0.069890 | 0.058845 | 0.018750 | 00:19 | . Our model only has a 1.9% error rate! Not bad! Though it seems if I had done an extra epoch, the error rate may have gone down to 1.3%, judging by the previous epochs&#39; error rates. . Visualizing mistakes . We can visualize the mistakes the model is making by a confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . 3 grizzly bears were misclassified as black bears. . Let us see where the errors are occurring, so we can determine if they are due to a dataset problem or a model problem. . To do this, we will sort images by their loss. . interp.plot_top_losses(5, nrows=1) . Data cleaning . The intuitive approach to data cleaning is to do it before training the model. However, a trained model can also help us clean the data. For example, we could see some mislabaled bears in the above cases. . fastai includes a GUI that allows you to choose a category/label and their associated training and validation sets and view the highest-loss images in order so that you can select images for removal or relabeling. . cleaner = ImageClassifierCleaner(learn) cleaner . ImageClassifierCleaner does not actually delete or relabel. It just returns the indices that are to be deleted or relabeled. . for index in cleaner.delete(): cleaner.fns[index].unlink() # Relabel images selected for relabeling. for index, category in cleaner.change(): shutil.move(str(cleaner.fns[index]), path/category) . We can now retrain and a better performance should be expected. . Saving the model . A model consists of two parts: the architecture and the parameters. . When we use the export() method, both of these are saved. . This method also saves the definition of your DataLoaders. This is done so that you do not have to redefine how to transform your data to use your model in production. . fastai uses your validation set DataLoader by default, so the data augmentation will not be applied, which is generally what you want. . The export() method creates a file named &quot;export.pkl&quot;. . learn.export() . Let us check that the file exists. . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . If you wish to deploy an app, this is the file you will need. . Loading the model for inference . Now obviously we do not need to load the model as we already have the learner variable. However, I will do so here for the sake of practice lol. . learn_inf = load_learner(path/&#39;export.pkl&#39;) . We generally do inference for a single image at a time. . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorBase(1), TensorBase([1.4230e-06, 1.0000e+00, 3.9502e-08])) . Three things have been returned: the predicted category, the index of the predicted category, and the probabilities of each category. . The order of each category is based of the order of the vocab of the DataLoaders; that is, the stored tuple of all possible categories. . The DataLoaders can be accessed as an attribute of the Learner. . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . A word about CNNs . Since the ResNet18 architecture is a sort of CNN, I will explain why CNNs work so well for images. I do not have a full understanding, but this is what I understand so far. . Essentially, each neuron in a layer is given the exact same weights and all neurons are input different data. This is so that all neurons fire upon detecting the same patterns. . By all neurons, in a layer, being given the exact same weights, it allows those neurons to all be triggered by the same pattern. This is why CNNs are really good at detecting objects in various patterns, orientations, shapes, positions, and so on. . Conclusion . Well then, that wraps up my first deep learning model! I have to say, it is much easier than I thought to implement a model. You do not need to go into the nitty gritty details in artificial intelligence to be able to implement them. A high level understanding will suffice. It is like playing a sport: you do not need to understand the physics to be able to play it. . Right now I am creating my own classifier that will classify cars into their company types. . If you have any comments, suggestions, or corrections, please do let me know down in the comment section below! .",
            "url": "https://forbo7.github.io/ForBlog/fastai/image%20classification/cnn/2022/05/28/bear_classifier_model.html",
            "relUrl": "/fastai/image%20classification/cnn/2022/05/28/bear_classifier_model.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "How to Approach Creating AI Models",
            "content": ". Introduction . How you approach making your model is crucial. Articial Intelligence is not about creating models; it is only a tiny part of it. It is 80% problem solving and 20% implementing (I would not be suprised if it actually followed the 80-20 rule1). . It is like the scientific method in a sense. Sure, you can create a really accurate, well functioning experiment. But what use is it if you do not approach conducting the experiment and analyzing the results in a well defined manner? . One highly successful approach is the Drivetrain Approach, created by fast.ai course creater Jeremy Howard along with his collegues Margit Zwemer and Mike Loukides. Their official blogpost on this approach goes into much detail and can be read in full here. The goal of this approach is to not use data just to generate more data (that is in the form of predictions), but to use data to also generate actionable outcomes. . Over here, I’ll be providing a short overview of my understanding of this approach (from the fast.ai course) and will be applying it to the final project I did in the University of Helsinki’s and Reaktor’s Elements of AI course (I highly highly recommend this course; it gives a really good primer into AI). . Overview of the Drivetrain Approach . There are four main steps to this process: . Define the objective | Consider your possible actions | Consider your data | Create the models | . Define the objective . Write out what you are really trying to achieve. What is your goal? . Consider your actions . Think about what actions you can take to achieve your objective. . Also think about what would happen if you did those actions. What would happen if did x? Would y really be a good idea? What if z worked really well? What if z worked really bad? . Consider your data . Think about the data you already have and how it can be used. Think about any further data that is needed and how it could be collected. . Create the model . Created models that produce the best actions that in turn produce the best results in terms of your objective. . Endangered Language Chatbot . The final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would. I ended up creating an overview for how a chatbot could be created to preserve endangered languages. . Now that I think of it, what the project asked me to do is one sort of approach to creating AI methods, though more on the why side. You can view my project overview here. . Define the objective . The objective of creating such a chatbot is to preserve languages that are endangered or extinct. This would help preserve different histories and cultures, as well as humanity’s diversity. . Consider your actions . Probably, a new sort of NLP architecture would have to be created primarily due to the lack of data there would be for these languages. . Alternatively, existing methods could be applied, but there would be varying degrees of success depending on how much data is available for a language. . Consider your data . The main source of data for training such models would corpuses of text. However, this itself is a problem since many endangered or extinct languages do not have enough written text that would achieve the level of performance required to have somewhat of a good conversation with the bot. Further troubles arise for those languages that are speech only; you would need to have audio recordings, which would be extremely difficult for a language that is endangered or nigh impossible for those that are extinct. . The main solution to these problems that comes to mind is literal manpower and brute force. That is, humans would have to manually create masses of text or speech that the model can then be trained on to “learn” the langauge. . Create the model . Create a model that speaks as accurately as a native of the endangered language so that any person interacting with the chatbot gets a vivid idea of the language, its culture, and its people. . Conclusion . This concludes my understanding and explanation of one such approach to creating models, along with an attempted example. Approaches are crucial: you can have all the best tools and the most accurate models in the world, but they are worthless if not correctly approached, used, and applied. . If you have any comments, suggestions, or corrections, please do post them down in the comment section below! . Footnotes . The 80/20 Rule, also known as the Pareto Principle &#8617; . |",
            "url": "https://forbo7.github.io/ForBlog/model%20deployment/drivetrain%20approach/2022/05/27/How-to-Approach-Creating-AI-Models.html",
            "relUrl": "/model%20deployment/drivetrain%20approach/2022/05/27/How-to-Approach-Creating-AI-Models.html",
            "date": " • May 27, 2022"
        }
        
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "Artstation",
          "content": "Check out my ArtStation where I upload my projects and works relating to 3D computer graphics. .",
          "url": "https://forbo7.github.io/ForBlog/Artstation.html",
          "relUrl": "/Artstation.html",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "GitHub",
          "content": "Check out my GitHub where you can see various coding projects that I have dabbled including game development and my ventures into AI. .",
          "url": "https://forbo7.github.io/ForBlog/GitHub.html",
          "relUrl": "/GitHub.html",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About Me",
          "content": "Want to contact me? . Email me at salmananaqvii+forblog@gmail.com. . I’m ForBo7: an approachable, adaptable, open-minded, curious individual who’s been to seventeen countries and lived in three. I’m a science and technology geek, space nerd, and currently dabble in AI. 3D computer graphics is my hobby and learning is my passion. .",
          "url": "https://forbo7.github.io/ForBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://forbo7.github.io/ForBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}